{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64fbccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca311ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CtQHiNgd0IgkrhzhpUE9BMQfbFMGB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1767325114, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "question = \"What is the capital of France?\"\n",
    "openai = OpenAI(\n",
    "    api_key=os.getenv(\"OPEN_AI_KEY\")\n",
    ")\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "095887ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9bfba29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f96eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65cee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254e4866",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m evaluator_system_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are an evaluator that decides whether a response to a question is acceptable. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33mYou are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms latest response is acceptable quality. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[33mThe Agent is playing the role of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mname\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and is representing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m on their website. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33mThe Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33mThe Agent has been provided with context on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in the form of their summary and LinkedIn details. Here\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the information:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m evaluator_system_prompt += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m## Summary:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msummary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m## LinkedIn Profile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mlinkedin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m evaluator_system_prompt += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWith this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e63a1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
